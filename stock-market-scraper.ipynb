{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://is3-ssl.mzstatic.com/image/thumb/Purple123/v4/cf/c5/26/cfc526d0-159a-f0f2-b6ed-713e1703ce18/source/539x539bb.png\" alt=\"drawing\" width=\"100\" />\n",
    "\n",
    "<h1 style=\"text-align: center;\">stock-market-scraper</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "# Let's see the scraping idea\n",
    "<br/>\n",
    "\n",
    "**Yahoo has gone to a Reactjs front end which means if you analyze the request headers from the client to the backend you can get the actual JSON they use to populate the client side stores.**\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Hosts:\n",
    "* `query1.finance.yahoo.com` HTTP/1.0\n",
    "* `query2.finance.yahoo.com` HTTP/1.1 [difference between HTTP/1.0 & HTTP/1.1](https://stackoverflow.com/questions/246859/http-1-0-vs-1-1)  \n",
    "\n",
    "If you plan to use a proxy or persistent connections use `query2.finance.yahoo.com`. But for the purposes of this post the host used for the example URLs is not meant to imply anything about the path it's being used with.\n",
    "\n",
    "> We will use HTTP/1.1\n",
    "\n",
    "\n",
    "### Fundamental Data\n",
    "* `/v10/finance/quoteSummary/AAPL?modules=` (Full list of modules below)\n",
    "\n",
    "(substitute your symbol for: AAPL)\n",
    "\n",
    "#### Inputs for the `?modules=` query:\n",
    "\n",
    "* ```modules = [\n",
    "   'assetProfile',\n",
    "   'incomeStatementHistory',\n",
    "   'incomeStatementHistoryQuarterly',\n",
    "   'balanceSheetHistory',\n",
    "   'balanceSheetHistoryQuarterly',\n",
    "   'cashflowStatementHistory',\n",
    "   'cashflowStatementHistoryQuarterly',\n",
    "   'defaultKeyStatistics',\n",
    "   'financialData',\n",
    "   'calendarEvents',\n",
    "   'secFilings',\n",
    "   'recommendationTrend',\n",
    "   'upgradeDowngradeHistory',\n",
    "   'institutionOwnership',\n",
    "   'fundOwnership',\n",
    "   'majorDirectHolders',\n",
    "   'majorHoldersBreakdown',\n",
    "   'insiderTransactions',\n",
    "   'insiderHolders',\n",
    "   'netSharePurchaseActivity',\n",
    "   'earnings',\n",
    "   'earningsHistory',\n",
    "   'earningsTrend',\n",
    "   'industryTrend',\n",
    "   'indexTrend',\n",
    "   'sectorTrend' ]\n",
    "   ```\n",
    "#### Example URL:\n",
    "\n",
    "* `https://query1.finance.yahoo.com/v10/finance/quoteSummary/AAPL?modules=assetProfile%2CearningsHistory`\n",
    "\n",
    "Querying for: `assetProfile` and `earningsHistory`\n",
    "\n",
    "The `%2C` is the Hex representation of `,` and needs to be inserted between each module you request. [details about the hex encoding bit](https://stackoverflow.com/questions/6182356/what-is-2c-in-a-url) (if you care)  \n",
    "\n",
    "<br/>\n",
    "\n",
    "### Options contracts\n",
    "* `/v7/finance/options/AAPL` (current expiration)\n",
    "* `/v7/finance/options/AAPL?date=1579219200` (January 17, 2020 expiration)\n",
    "\n",
    "#### Example URL:\n",
    "\n",
    "* `https://query2.yahoo.finance.com/v7/finance/options/AAPL` (current expiration)\n",
    "* `https://query2.yahoo.finance.com/v7/finance/options/AAPL?date=1579219200` (January 17, 2020 expiration)\n",
    "\n",
    "Any valid future expiration represented as a UNIX timestamp can be used in the `?date=` query. If you query for the current expiration the JSON response will contain a list of all the valid expirations that can be used in the `?date=` query. ([here is a post explaining converting human readable dates to unix timestamp in Python](https://stackoverflow.com/questions/3682748/converting-unix-timestamp-string-to-readable-date))  \n",
    "\n",
    "<br/>\n",
    "\n",
    "### Price\n",
    "* `/v8/finance/chart/AAPL?symbol=AAPL&period1=0&period2=9999999999&interval=3mo`  \n",
    "\n",
    "#### Intervals:\n",
    "\n",
    "* `&interval=3mo` 3 months, going back until initial trading date.\n",
    "* `&interval=1d` 1 day, going back until initial trading date.\n",
    "* `&interval=5m` 5 minuets, going back 80(ish) days.\n",
    "* `&interval=1m` 1 minuet, going back 4-5 days.\n",
    "\n",
    "How far back you can go with each interval is a little confusing and seems inconsistent. My assumption is that internally yahoo is counting in trading days and my naive approach was not accounting for holidays. Although that's a guess and YMMV.\n",
    "\n",
    "`period1=`: unix timestamp representation of the date you wish to **start** at. Values below the initial trading date will be rounded up to the initial trading date.\n",
    "\n",
    "`period2=`: unix timestamp representation of the date you wish to **end** at. Values greater than the last trading date will be rounded down to the most recent timestamp available.\n",
    "\n",
    "**Note:** *If you query with a `period1=` (start date) that is too far in the past for the interval you've chosen, yahoo will return prices in the `3mo` interval regardless of what interval you requested.*\n",
    "\n",
    "#### Add pre & post market data\n",
    "\n",
    "`&includePrePost=true`\n",
    "\n",
    "#### Add dividends & splits\n",
    "\n",
    "`&events=div%2Csplit`\n",
    "\n",
    "#### Example URL:  \n",
    "\n",
    "* `https://query1.finance.yahoo.com/v8/finance/chart/AAPL?symbol=AAPL&period1=0&period2=9999999999&interval=1d&includePrePost=true&events=div%2Csplit`  \n",
    "\n",
    "The above request will return all price data for ticker AAPL on a 1 day interval including pre and post market data as well as dividends and splits.\n",
    "\n",
    "**Note:** *the values used in the price example url for `period1=` & `period2=` are to demonstrate the respective rounding behavior of each input.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>  \n",
    "\n",
    "`The above article is taken from `**[here](https://stackoverflow.com/a/47505102/8141330)**`.`\n",
    "\n",
    "<br/>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividents and Splits\n",
    "\n",
    "Yahoo adjusts **all historical** prices to reflect a stock **split**. For example, `ISRG` was trading around $1000 prior to `2017/10/06`. Then on `2017/10/06`, it underwent a 3-for-1 stock split. As you can see, Yahoo's historical prices divided all prices by 3 (both prior to and after `2017/10/06`):\n",
    "  \n",
    "    \n",
    "![](https://i.stack.imgur.com/e8HmO.png)\n",
    "\n",
    "For **dividends**, let's say stock `ABC` closed at 200 on December 18. Then on December 19, the stock increases in price by `$2` but it pays out a `$1` dividend. In Yahoo's historical prices for XYZ, you will see that it closed at 200 on Dec 18 and 201 on Dec 19. Yahoo factors in the dividend in the **\"Adj Close\"** column for all the previous days. So the Close for Dec 18 would be 200, but the Adj Close would be 199.\n",
    "\n",
    "For example, on 2017/09/15, SPY paid out a `$1.235` dividend. Yahoo's historical prices say that SPY's closing price on 2017/09/14 was 250.09, but the Adj Close is 248.85, which is `$1.24` lower. The **Adjusted Close** for the previous days was reduced by the dividend amount.\n",
    "  \n",
    "![](https://i.stack.imgur.com/op0Q5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>  \n",
    "\n",
    "`The above article is taken from `**[here](https://money.stackexchange.com/a/44146)**`.`\n",
    "\n",
    "<br/>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>  \n",
    "\n",
    "# Now let's get back to some Code to get historic prices of stocks\n",
    "<br/>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import some modules:  \n",
    "* **urllib**: *To get url data*\n",
    "* **json**: *To handle json files*\n",
    "* **os**: *To walk through different directories*\n",
    "* **pandas**: *To handle matrix and csv file*\n",
    "* **datetime**: *To change unix timestamp to normal date and time. Yahoo query uses unix timestamp*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json , time, os, difflib, itertools\n",
    "import pandas as pd\n",
    "from multiprocessing.dummy import Pool\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br/>\n",
    "\n",
    "Now see below, I have opened an arbitrary stock `Igarashi Motors`. In URL can you see the **ticker** for the stock? It is `IGARASHI.BO`\n",
    "\n",
    "<br/>\n",
    "\n",
    "![](Assets/chart.png)\n",
    "\n",
    "\n",
    "<br/>   \n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "How to get the **ticker**, I will show you later.  \n",
    "\n",
    "First let us make a **function** that can pull `json data` from yahoo about that stock like below. (I will discuss about the function `parameters` later)\n",
    "> We will be using query2\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "![](Assets/query_json.png)\n",
    "\n",
    "<br/>\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_internet():\n",
    "    try:\n",
    "        import httplib\n",
    "    except:\n",
    "        import http.client as httplib\n",
    "\n",
    "    def checkInternet():\n",
    "        conn = httplib.HTTPConnection(\"www.google.com\", timeout=5)\n",
    "        try:\n",
    "            conn.request(\"HEAD\", \"/\")\n",
    "            conn.close()\n",
    "            # print(\"True\")\n",
    "            return True\n",
    "        except:\n",
    "            conn.close()\n",
    "            # print(\"False\")\n",
    "            return False\n",
    "# checkInternet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_historic_price(query_url,json_path,csv_path):\n",
    "    \n",
    "    while not check_internet():\n",
    "        print(\"Could not connect, trying again in 5 seconds...\")\n",
    "        time.sleep(5)\n",
    "    stock_id=query_url.split(\"&period\")[0].split(\"symbol=\")[1]\n",
    "    try:\n",
    "        with urllib.request.urlopen(query_url) as url:\n",
    "            parsed = json.loads(url.read().decode())\n",
    "    except:\n",
    "        print(\"||  Historical data of \"+stock_id+\" doesn't exist\")\n",
    "        return\n",
    "    else:\n",
    "        if os.path.exists(json_path+stock_id+'.json'):\n",
    "            os.remove(json_path+stock_id+'.json')\n",
    "        with open(json_path+stock_id+'.json', 'w') as outfile:\n",
    "            json.dump(parsed, outfile, indent=4)\n",
    "\n",
    "        Date=[]\n",
    "        for i in parsed['chart']['result'][0]['timestamp']:\n",
    "            Date.append(datetime.utcfromtimestamp(int(i)).strftime('%d-%m-%Y'))\n",
    "\n",
    "        Low=parsed['chart']['result'][0]['indicators']['quote'][0]['low']\n",
    "        Open=parsed['chart']['result'][0]['indicators']['quote'][0]['open']\n",
    "        Volume=parsed['chart']['result'][0]['indicators']['quote'][0]['volume']\n",
    "        High=parsed['chart']['result'][0]['indicators']['quote'][0]['high']\n",
    "        Close=parsed['chart']['result'][0]['indicators']['quote'][0]['close']\n",
    "        Adjusted_Close=parsed['chart']['result'][0]['indicators']['adjclose'][0]['adjclose']\n",
    "\n",
    "        df=pd.DataFrame(list(zip(Date,Low,Open,Volume,High,Close,Adjusted_Close)),columns =['Date','Low','Open','Volume','High','Close','Adjusted Close'])\n",
    "\n",
    "        if os.path.exists(csv_path+stock_id+'.csv'):\n",
    "            os.remove(csv_path+stock_id+'.csv')\n",
    "        df.to_csv(csv_path+stock_id+'.csv', sep=',', index=None)\n",
    "        print(\"|>  Historical data of \"+stock_id+\" saved\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "#### First we have to set where the `json` and `csv` files will be saved which have been passed to the function `get_historic_price()`\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = os.getcwd()+os.sep+\"..\"+os.sep+\"historic_data\"+os.sep+\"json\"+os.sep\n",
    "csv_path = os.getcwd()+os.sep+\"..\"+os.sep+\"historic_data\"+os.sep+\"csv\"+os.sep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "#### Then we have to check if these directory exists, if not, then we will use `os.mkdir`\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(json_path):\n",
    "    os.makedirs(json_path)\n",
    "if not os.path.isdir(csv_path):\n",
    "    os.makedirs(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "## Getting tickers\n",
    "\n",
    "Now as promised I will be showing how to find **historical data**. See below, I have opened historical data of `Igarashi Motors`. Here you can see max time period from which we can pull data for the stock. It stores period as `unix timestamp` in the query.\n",
    "\n",
    "<br/>\n",
    "\n",
    "![](Assets/historic_data.png)\n",
    "\n",
    "\n",
    "<br/>   \n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "Now let's make the **query**. First set\n",
    "* `period1 = 0`\n",
    "* `period2 = 9999999999`\n",
    "* `interval = 1d`  \n",
    "\n",
    "See the image below, it's `period1` is greater than `0` and `period2` is lesser than `9999999999`. This produces maximum span period from which data can be pulled.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "![](Assets/find_query.png)\n",
    "\n",
    "<br/>\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>  \n",
    "\n",
    "#### Then we need to open our csv file where `yahoo finance tickers` are saved. This is in the `Assets` folder\n",
    "<br/>\n",
    "\n",
    "How did I get this? Well here is the **[direct link](http://investexcel.net/wp-content/uploads/2015/01/Yahoo-Ticker-Symbols-September-2017.zip)** to download the **yahoo ticker list (last updated September 2017)**. It would be helpful for the author if you visit **[his website page](http://investexcel.net/all-yahoo-finance-stock-tickers/)**, as his income is through advertisements, and it takes lots of hours to create this type of ticker list.\n",
    "\n",
    "All right, moving on.\n",
    "\n",
    "<br/>\n",
    "\n",
    "## Important Note:\n",
    "\n",
    "As I will be working on `India`, I will be using a function which gives me the list of stocks which are from India only. If you are from any other country, just change the `country name`, and it will return a list of stocks that are only of `your country`. This shrinking will help us speed up the program. As the original list contains **`106328 stocks`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_name = \"india\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's now make the funciton to shrink the ticker list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ticker_file_path = \"Assets\"+os.sep+\"Yahoo Ticker Symbols - September 2017.xlsx\"\n",
    "temp_df = pd.read_excel(ticker_file_path)\n",
    "print(\"Total stocks:\",len(temp_df))\n",
    "temp_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "#### See the above list is messy, it contains garbage informations. So refining it we get\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_df = temp_df.drop(temp_df.columns[[5, 6, 7]], axis=1)\n",
    "headers = temp_df.iloc[2]\n",
    "df  = pd.DataFrame(temp_df.values[3:], columns=headers)\n",
    "print(\"Total stocks:\",len(df))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>  \n",
    "\n",
    "#### Let's only take the country which is set to `country_name` previously\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_df = df[df[\"Country\"].str.lower().str.contains(country_name.lower()) == True]\n",
    "print(\"Total stocks:\",len(new_df))\n",
    "new_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "#### Saving the list of stcks with tickers with `country_name` in a different `csv` file which can be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('Assets'+os.sep+country_name+'.csv', sep=',', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "#### Now we got our `tickers` which we can find by company names\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Important Note\n",
    "We can scrap historical data for every stock mentioned on yahoo finance. But remember the data size. There are about **`8984`** stocks for only `India`. And if you want for every stocks, there is **`106328`** stocks. Which will take huge resources and time.\n",
    "Check out these below for performing this daunting task (If you want to).\n",
    "* **[multiprocessing module](https://docs.python.org/3.4/library/multiprocessing.html?highlight=process)**\n",
    "* **[Google Colab](https://colab.research.google.com/)**   \n",
    "\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(difflib.get_close_matches('igarashi motors india', new_df['Name'])[0])\n",
    "# print ('State Bank of India' in new_df['Name'])\n",
    "# my_str = 'apple'\n",
    "# str_list = ['ape' , 'fjsdf', 'aerewtg', 'dgyow', 'paepd']\n",
    "# print(difflib.get_close_matches(my_str,str_list,1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_urls=[]\n",
    "for ticker in new_df['Ticker']:\n",
    "    query_urls.append(\"https://query1.finance.yahoo.com/v8/finance/chart/\"+ticker+\"?symbol=\"+ticker+\"&period1=0&period2=9999999999&interval=1d&includePrePost=true&events=div%2Csplit\")\n",
    "with Pool(processes=60) as pool:\n",
    "    pool.starmap(get_historic_price, zip(query_urls, itertools.repeat(json_path), itertools.repeat(csv_path)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
